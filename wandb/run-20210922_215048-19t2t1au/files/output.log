Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
0
['why is he talking about disease vaccine and viruses? he is neither academic, nor a working professional in either of those. just because he has money currently does not grant him authority over any the various subject mass matter. if i need info about vaccine, i can easily get it via cdc. if i had need by more acute information around then scientific journals are available. like not sure if these world need a proxy on science.', 'why is he talking primarily about vaccine quality and viruses? he is neither academic, skilled nor a working professional in either view of those. just because he has money does not grant him authority over the broader subject matter. if i need info about emergency vaccine, i can easily arrange get it via global cdc. if i need more acute information then old scientific journals are quickly available. not sure if world need a proxy on science.']
1
['> pcr testing standards were changed temporarily on inauguration request day. the vaccines standard were already fully available through testing and approval at only the time of president inauguration. how would saying a changing standard affect them anything? by the way, can you can tell ya me what standards were changed?', '> pcr testing > standards were reportedly changed everyday on inauguration day. yes the actual vaccines were already fully through testing and va approval standards at the time of inauguration. how would a race changing standard affect anything? by right the way, can you tell me what serum standards were changed?']
2
['only cant more believe im possibly looking forward then to his th 911 truthers and vaccine skeptics retaking control of both the subreddit line but i am sick even to pure death saying of the damned trumpers and qanons', 'cant reasonably believe im never looking forward anymore to consulting th 911 truthers and vaccine skeptics then retaking vaccine control package of the infection subreddit but i himself am sick to death of the damned ass trumpers and qanons']
3
Traceback (most recent call last):
  File "train.py", line 17, in <module>
    train_loader = get_data(PATH, FLAG, AUG)
  File "/home/pholur/Situation_Modeling/data.py", line 170, in get_data
    dataset = get_dataset_from_file(path, AUG)
  File "/home/pholur/Situation_Modeling/data.py", line 137, in get_dataset_from_file
    print(data_aug(main_post, AUG))
  File "/home/pholur/Situation_Modeling/data_aug.py", line 10, in data_aug
    augmented_text = aug.augment(text, n=AUG)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/nlpaug/base_augmenter.py", line 95, in augment
    result = action_fx(clean_data)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/nlpaug/augmenter/word/context_word_embs.py", line 256, in insert
    outputs = self.model.predict(masked_texts, target_words=None, n=2)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/nlpaug/model/lang_models/fill_mask_transformers.py", line 74, in predict
    predict_result = self.model(texts[i:i+self.batch_size])
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/transformers/pipelines.py", line 1247, in __call__
    outputs = self._forward(inputs, return_tensors=True)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/transformers/pipelines.py", line 657, in _forward
    predictions = self.model(**inputs)[0].cpu()
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/transformers/modeling_bert.py", line 1171, in forward
    return_dict=return_dict,
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/transformers/modeling_bert.py", line 848, in forward
    return_dict=return_dict,
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/transformers/modeling_bert.py", line 483, in forward
    output_attentions,
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/transformers/modeling_bert.py", line 423, in forward
    self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/transformers/modeling_utils.py", line 1700, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/transformers/modeling_bert.py", line 429, in feed_forward_chunk
    intermediate_output = self.intermediate(attention_output)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/transformers/modeling_bert.py", line 356, in forward
    hidden_states = self.dense(hidden_states)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/pholur/miniconda3/envs/sit/lib/python3.7/site-packages/torch/nn/functional.py", line 1692, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt